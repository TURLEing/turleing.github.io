<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="天泽龟的龟壳屋">
  <link 
    rel="icon" 
    href="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
  <title>CS224N 学习随笔【二】</title>
  
    
      <meta 
        property="og:title" 
        content="CS224N 学习随笔【二】">
    
    
      <meta 
        property="og:url" 
        content="https://tzturtle.moe/2024/11/12/cs224n-2/index.html">
    
    
      <meta 
        property="og:img" 
        content="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
    
    
      <meta 
        property="og:img" 
        content="&lt;p&gt;LLM is all you need.&lt;/p&gt;">
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2024-11-12">
      <meta 
        property="og:article:modified_time" 
        content="2024-11-19">
      <meta 
        property="og:article:author" 
        content="天泽龟">
      
        
          <meta 
            property="og:article:tag" 
            content="NLP">
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
    
    <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.4.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img" 
          src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">天泽龟的龟壳屋</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
      <a 
        href="/bangumis" 
        class="navbar-menu-item">
        
          番剧
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      CS224N 学习随笔【二】
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2024-11-12T09:47:34.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2024-11-12</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/categories/%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0/" 
          class="post-meta-link">
          专业学习
        </a>
      
    
    
      <span class="dot"></span>
      <span>2.5k 字</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/tags/NLP/" 
            class="post-meta-link">
            NLP
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <p>LLM is all you need.</p>
<span id="more"></span>
<h2 id="lecture-10-instruction-finetuning-and-rlhf"><a class="markdownIt-Anchor" href="#lecture-10-instruction-finetuning-and-rlhf"></a> Lecture 10: Instruction Finetuning, and RLHF</h2>
<p>预训练可以通过作为参数初始化来提升 NLP 应用的效果，它让 LLM 成为了 “世界模型” ！然而，仅仅进行语言建模并不等于能有效地帮助用户，这就是 Finetuning Paradigm 出现的原因。</p>
<h3 id="指令微调instruction-finetuningflan-t5"><a class="markdownIt-Anchor" href="#指令微调instruction-finetuningflan-t5"></a> 指令微调（Instruction finetuning，Flan-T5）</h3>
<p>指令微调指的是：收集包含多种任务的【指令，输出】对，并用这些数据微调语言模型。研究已经证明，在一组表述为指令的数据集上微调语言模型可以提高模型性能和对未知任务的泛化能力。在本文中，作者探索了指令微调，特别关注：</p>
<ul>
<li>(1) 缩放任务数量；</li>
<li>(2) 缩放模型大小；</li>
<li>(3) 链式思维数据微调；</li>
</ul>
<p>论文发现，在上述方面进行指令微调可以显着提高各种模型（PaLM、T5、U-PaLM）、提示设置（零样本、少样本、CoT）和评估基准（MMLU、BBH）。</p>
<p>指令是一组数据集，一组用指令表达的任务。<strong>使用指令数据进行微调使模型能够更好地响应指令，并减少对样本的需求。</strong> 一般的发现是，微调的效果与任务数量和模型大小成比例。两种关系都是正相关的（模型越大、训练任务越多，指令在少样本和零样本示例中性能提升更多）。</p>
<p>这项研究还使用思维链 (CoT) 数据对模型进行微调，CoT 数据指令微调的另一个重要好处是解锁零样本推理。这测试了模型在没有 CoT 的少量示例的情况下产生自己的推理技能的能力。</p>
<p>然而正如预训练模型一样，数据和模型规模成为了指令微调成功的关键。例如，Super Natural Instructions 数据集。这个数据集包含了超过1600个不同的任务和超过300万个示例。这样的大规模数据集可以帮助我们的模型学习到更广泛的语言模式和任务特定的知识。</p>
<p>同时，一个新的 Benchmark 也被提出来。Massive Multitask Language Understanding（MMLU）旨在衡量语言模型在57个不同的、知识密集型任务上的表现。这些任务覆盖了广泛的领域，包括但不限于历史、地理、数学、科学等，它们要求模型不仅要有强大的语言理解能力，还要有广泛的世界知识和推理能力。</p>
<p>MMLU的出现，为我们提供了一个全新的视角来评估语言模型的性能。传统的语言模型评估往往集中在单一任务上，比如 GLUE 或者 SQuAD，这些评估虽然重要，但它们并不能全面反映一个模型的多任务处理能力。</p>
<p>最后，说是指令微调，但感觉其实更像是我们现在所说的监督微调 (SFT)。这种方法简单直白，也易泛化到没见过的任务上。然而，监督指令微调也有几点局限性：</p>
<ol>
<li><strong>数据收集成本高昂：</strong> 为了训练和评估模型，我们需要大量的正确答案或期望输出，这些数据通常需要人工标注，这既耗时又昂贵。</li>
<li><strong>开放性任务没有标准答案：</strong> 比如编写故事并没有一个“正确”的答案，可以有无数种创意和情节发展，每种都是独特的，对于这类人物的评估也有一定困难。</li>
<li>即使进行了指令微调，<strong>语言模型的目标与满足人类偏好的目标之间仍然存在不匹配</strong>。语言模型通常被优化以生成在统计上可能的输出，而不是那些最能符合人类期望和偏好的输出。</li>
</ol>
<p>此外，其实指令微调的数据也并非需要很多 — — 研究表明大型语言模型中的几乎所有知识都是在预训练期间学习的，并且只需要有限的指令调优数据来教模型产生高质量的输出。</p>
<p><img src="image-20241112183540281.png" alt="image-20241112183540281" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="image-20241112183540281.png" class="lozad post-image"></p>
<br>
<h3 id="人类反馈强化学习rlhf"><a class="markdownIt-Anchor" href="#人类反馈强化学习rlhf"></a> 人类反馈强化学习（RLHF）</h3>
<p>为了对齐人类期望，我们引入了强化学习的概念。具体来说，对于语言模型（LM）生成的样本 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">s</span></span></span></span>，我们假设有一种方法可以获得人类对这个样本的奖励 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">R_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，数值越高表示越好。我们的目标是最大化从语言模型中抽取的样本的预期奖励 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">E[R(s)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>。这意味着我们可以直接从人类那里获得关于模型输出质量的反馈。</p>
<p>将 RL 与 LM 结合是一个比较新的领域，得益于新的RL算法的发展，特别是那些适用于大型神经网络模型的算法（例如PPO，由Schulman等人在2017年提出）。</p>
<p>为了最大化预期奖励 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">E[R(s)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>，我们可以使用<strong>梯度上升</strong>的方法更新模型参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">θ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> ：</p>
<p class='katex-block katex-error' title='Error: Font metrics not found for font: .'>𝜃_{t+1}=𝜃_t+𝛼∇_{\theta_t} 𝔼_{\hat s\sim p_{\theta_t}(s)} [R(\hat s)]
</p>
<p>解释一下就是，对于当前模型（参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>）生成的一个样本 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>s</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">s</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span></span></span></span></span></span></span>（或者说是 response）。我们想最大化人类奖励的期望 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span>，于是朝着梯度上升的方向更新 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> .</p>
<p>最简单的方式是通过 <strong>蒙特卡洛采样</strong> 得到若干条样本，并更新参数，使得奖励高的样本更容易出现。</p>
<p><img src="image-20241112154423461.png" alt="image-20241112154423461" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="image-20241112154423461.png" class="lozad post-image"></p>
<br>
<h3 id="how-do-we-model-human-preferences"><a class="markdownIt-Anchor" href="#how-do-we-model-human-preferences"></a> How do we model human preferences?</h3>
<p>现在对于任意的、不可微分的奖励函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span></span></span></span> ，我们可以训练我们的语言模型来最大化预期奖励，感觉无敌了。但问题是让人类给样本打分实在是太慢，并且也很浪费钱。</p>
<p>为了克服这个问题，我们可以将人类偏好的建模视为一个独立的自然语言处理（NLP）问题。具体来说，我们可以训练一个 <strong>奖励模型</strong> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>M</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">RM(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span></span></span></span>，该模型能够根据已有的标注数据，预测人类的偏好。这样我们就可以在训练语言模型时采用这个模型的奖励输出，而不是直接依赖于人类的反馈。</p>
<p>除此之外，RLHF 还面临着噪声和道德问题。一般来讲，我们不直接要求人类给出具体的评分，而是让他们在两个输出之间进行比较。这种方法被称为 <strong>成对比较（pairwise comparisons）</strong> ，它通常更加可靠，因为：</p>
<ol>
<li><strong>简化决策</strong>：成对比较简化了评价者的任务，他们只需要判断两个选项中哪一个更好，而不是给出一个具体的评分。</li>
<li><strong>减少噪声</strong>：由于比较是基于相对优劣，这有助于减少单个评价中的随机噪声。</li>
<li><strong>校准一致性</strong>：成对比较可以帮助评价者建立更加一致的评价标准，因为他们可以直接看到比较的选项。</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="script">J</mi><mrow><mi>R</mi><mi>M</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Φ</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo>∼</mo><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>R</mi><mi>M</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><msub><mi>R</mi><mi>M</mi></msub><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex"> \mathcal{J}_{RM}(\Phi) = -\mathbb{E}_{(s, s&#x27;) \sim D}[\log \sigma(R_M(p(s)) - R_M(g(s&#x27;))]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.18472em;">J</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Φ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord">−</span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">s</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>这里的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="script">J</mi><mrow><mi>R</mi><mi>M</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Φ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{J}_{RM}(\Phi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.18472em;">J</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Φ</span><span class="mclose">)</span></span></span></span> 表示奖励模型的损失函数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mi>M</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R_M(p(s))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mi>M</mi></msub><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R_M(g(s&#x27;))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> 分别是模型对正样本 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mclose">)</span></span></span></span> 和负样本 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的评分。</p>
<br>
<h3 id="chatgpt-instruction-finetuning-rlhf-for-dialog-agents"><a class="markdownIt-Anchor" href="#chatgpt-instruction-finetuning-rlhf-for-dialog-agents"></a> ChatGPT: Instruction Finetuning + RLHF for dialog agents</h3>
<p><code>ChatGPT</code> 就是以上两种方法的集大成者：</p>
<ol>
<li>
<p>使用人类反馈强化学习 (RLHF) 训练了这个模型，使用与 <code>InstructGPT</code> 相同的方法，但在数据收集设置上略有不同。</p>
<p>我们使用监督微调训练了一个初始模型：人类提供了对话，他们在对话中同时扮演用户和 AI 助手。我们将新的对话数据集与 <code>InstructGPT</code> 数据集混合，并将其转换为对话格式。</p>
</li>
<li>
<p>为了创建强化学习的奖励模型，我们需要收集比较数据，这些数据由两个或多个按质量排名的模型响应组成。为了收集这些数据，我们采用了AI培训师与聊天机器人进行的对话。</p>
<p>我们随机选择了一个模型编写的消息，采样了几个替代完成，并让AI培训师对其进行排名。使用这些奖励模型，我们可以使用近端政策优化 (PPO) 微调模型。</p>
</li>
</ol>
<p>此外，在RLHF的研究中，由于获取大量的人类反馈既昂贵又耗时，许多研究工作开始探索使用模拟的GPT-4反馈作为人类反馈的替代品。根据  [Dubois et al 2023] 的研究，<code>ChatGPT</code> 喜欢一条条列表给你解释东西的习惯也可能是 RLHF 使然的结果。</p>
<br>
<h3 id="limitations-of-rl-rm-removing-the-rl-from-rlhf"><a class="markdownIt-Anchor" href="#limitations-of-rl-rm-removing-the-rl-from-rlhf"></a> Limitations of RL + RM &amp; Removing the ‘RL’ from RLHF</h3>
<p>然而，RLHF 仍面临着以下问题：</p>
<ul>
<li>
<p>**人类偏好也有可能是不可靠的。**人类反馈可能会受到个人偏见、情绪状态或当时情境的影响，导致反馈结果不稳定或不一致。</p>
</li>
<li>
<p><strong>“奖励黑客”（Reward Hacking）</strong> 也是一个常见问题。它指的是模型可能会学会奖励系统的偏好性，生成一些看似符合人类偏好但实际上没用的输出，以获得更高的奖励，但不能真正提高其性能。</p>
</li>
<li>
<p><strong>RLHF 的训练过程十分繁琐。</strong> 首先我们得让模型生成若干样本，然后再用 Reward Model 对其打分，打完分之后再优化 LM 的生成策略并不断迭代，这样做会使得过程非常冗杂，并且会产生上述“奖励黑客”的问题。</p>
</li>
</ul>
<p><img src="image-20241112172517487.png" alt="image-20241112172517487" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="image-20241112172517487.png" class="lozad post-image"></p>
<br>
<p>因此我们提出了 DPO（Direct Preference Optimization），旨在直接优化模型输出以符合人类的偏好。具体怎么实现的看隔壁 RL 笔记。</p>
<p><img src="image-20241112173459799.png" alt="image-20241112173459799" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="image-20241112173459799.png" class="lozad post-image"></p>
<br>
<h3 id="whats-next"><a class="markdownIt-Anchor" href="#whats-next"></a> What’s Next……</h3>

  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            天泽龟
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://tzturtle.moe/2024/11/12/cs224n-2/">
            https://tzturtle.moe/2024/11/12/cs224n-2/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
    <div class="nav-item-prev">
      <a 
        href="/2024/11/20/rl-3/" 
        class="nav-link">
        <i class="iconfont icon-left nav-prev-icon"></i>
        <div>
          <div class="nav-label">上一篇</div>
          
            <div class="nav-title">《动手学强化学习》学习笔记【三】 </div>
          
        </div>
      </a>
    </div>
  
  
    <div class="nav-item-next">
      <a 
        href="/2024/11/09/rl-2/" 
        class="nav-link">
        <div>
          <div class="nav-label">下一篇</div>
          
            <div class="nav-title">《动手学强化学习》学习笔记【二】 </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: '4CiiPBXpbjDnPvIIwfuEPEY6-gzGzoHsz',
        appKey: '3AQY35K3Laq9fLvTG2uOHDUT',
        placeholder: '留下你的评论...',
        path: window.location.pathname,
        avatar: 'identicon',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'false',
        highlight: true,
        recordIP: false,
        
        
        
        enableQQ: 'true',
        requiredFields: [],
      });
    };
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-10-instruction-finetuning-and-rlhf"><span class="toc-text"> Lecture 10: Instruction Finetuning, and RLHF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83instruction-finetuningflan-t5"><span class="toc-text"> 指令微调（Instruction finetuning，Flan-T5）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0rlhf"><span class="toc-text"> 人类反馈强化学习（RLHF）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-do-we-model-human-preferences"><span class="toc-text"> How do we model human preferences?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chatgpt-instruction-finetuning-rlhf-for-dialog-agents"><span class="toc-text"> ChatGPT: Instruction Finetuning + RLHF for dialog agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations-of-rl-rm-removing-the-rl-from-rlhf"><span class="toc-text"> Limitations of RL + RM &amp; Removing the ‘RL’ from RLHF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#whats-next"><span class="toc-text"> What’s Next……</span></a></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
    class="author-img" 
    alt="author avatar">

<p class="author-name">天泽龟</p>
<p class="author-description">天泽龟的龟是龟裂的龟哦。</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>52</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>5</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>13</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://space.bilibili.com/12645985">
          <i class="iconfont icon-bilibili society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://github.com/TURLEing">
          <i class="iconfont icon-github society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://turleing.github.io/about/">
          <i class="iconfont icon-mail society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-10-instruction-finetuning-and-rlhf"><span class="toc-text"> Lecture 10: Instruction Finetuning, and RLHF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83instruction-finetuningflan-t5"><span class="toc-text"> 指令微调（Instruction finetuning，Flan-T5）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0rlhf"><span class="toc-text"> 人类反馈强化学习（RLHF）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-do-we-model-human-preferences"><span class="toc-text"> How do we model human preferences?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chatgpt-instruction-finetuning-rlhf-for-dialog-agents"><span class="toc-text"> ChatGPT: Instruction Finetuning + RLHF for dialog agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations-of-rl-rm-removing-the-rl-from-rlhf"><span class="toc-text"> Limitations of RL + RM &amp; Removing the ‘RL’ from RLHF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#whats-next"><span class="toc-text"> What’s Next……</span></a></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/%E7%94%9F%E6%B4%BB%E5%88%86%E4%BA%AB/">
        <div class="categories-list-item">
          生活分享
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/">
        <div class="categories-list-item">
          算法竞赛
          <span class="categories-list-item-badge">12</span>
        </div>
      </a>
    
      <a href="/categories/%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          专业学习
          <span class="categories-list-item-badge">24</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          科研学习
          <span class="categories-list-item-badge">6</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A4%BE%E5%9B%A2%E6%8E%A8%E9%80%81/">
        <div class="categories-list-item">
          社团推送
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" 
        title="计算机组成原理">
        <div class="tags-list-item">计算机组成原理</div>
      </a>
    
      <a 
        href="/tags/%E9%9A%8F%E7%AC%94/" 
        title="随笔">
        <div class="tags-list-item">随笔</div>
      </a>
    
      <a 
        href="/tags/NLP/" 
        title="NLP">
        <div class="tags-list-item">NLP</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" 
        title="强化学习">
        <div class="tags-list-item">强化学习</div>
      </a>
    
      <a 
        href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" 
        title="操作系统">
        <div class="tags-list-item">操作系统</div>
      </a>
    
      <a 
        href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" 
        title="字符串">
        <div class="tags-list-item">字符串</div>
      </a>
    
      <a 
        href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" 
        title="分布式">
        <div class="tags-list-item">分布式</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" 
        title="大模型">
        <div class="tags-list-item">大模型</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F/" 
        title="多项式">
        <div class="tags-list-item">多项式</div>
      </a>
    
      <a 
        href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" 
        title="推荐系统">
        <div class="tags-list-item">推荐系统</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" 
        title="强连通分量">
        <div class="tags-list-item">强连通分量</div>
      </a>
    
      <a 
        href="/tags/%E7%BA%BF%E6%80%A7%E5%9F%BA/" 
        title="线性基">
        <div class="tags-list-item">线性基</div>
      </a>
    
      <a 
        href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" 
        title="博弈论">
        <div class="tags-list-item">博弈论</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-10-instruction-finetuning-and-rlhf"><span class="toc-text"> Lecture 10: Instruction Finetuning, and RLHF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83instruction-finetuningflan-t5"><span class="toc-text"> 指令微调（Instruction finetuning，Flan-T5）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0rlhf"><span class="toc-text"> 人类反馈强化学习（RLHF）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-do-we-model-human-preferences"><span class="toc-text"> How do we model human preferences?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chatgpt-instruction-finetuning-rlhf-for-dialog-agents"><span class="toc-text"> ChatGPT: Instruction Finetuning + RLHF for dialog agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations-of-rl-rm-removing-the-rl-from-rlhf"><span class="toc-text"> Limitations of RL + RM &amp; Removing the ‘RL’ from RLHF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#whats-next"><span class="toc-text"> What’s Next……</span></a></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-11-20</div>
        <a href="/2024/11/20/rl-3/"><div class="recent-posts-item-content">《动手学强化学习》学习笔记【三】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-11-12</div>
        <a href="/2024/11/12/cs224n-2/"><div class="recent-posts-item-content">CS224N 学习随笔【二】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-11-09</div>
        <a href="/2024/11/09/rl-2/"><div class="recent-posts-item-content">《动手学强化学习》学习笔记【二】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-10-28</div>
        <a href="/2024/10/28/parallel-training-survey/"><div class="recent-posts-item-content">【转载】浅谈大模型分布式训练并行技术</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020 -
          
          2024
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          天泽龟的龟壳屋
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer> 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton" 
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
    
      <script> 
        loadScript('/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
    
  </body>
</html>
