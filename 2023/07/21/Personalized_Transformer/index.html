<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="天泽龟的龟壳屋">
  <link 
    rel="icon" 
    href="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
  <title>记人生第一次论文复现以及微小的改进</title>
  
    
      <meta 
        property="og:title" 
        content="记人生第一次论文复现以及微小的改进">
    
    
      <meta 
        property="og:url" 
        content="https://tzturtle.moe/2023/07/21/Personalized_Transformer/index.html">
    
    
      <meta 
        property="og:img" 
        content="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
    
    
      <meta 
        property="og:img" 
        content="&lt;h2 id=&#34;前言&#34;&gt;&lt;a class=&#34;markdownIt-Anchor&#34; href=&#34;#前言&#34;&gt;&lt;/a&gt; 前言：&lt;/h2&gt;
&lt;p&gt;本篇博客的目的是详细总结 &lt;strong&gt;“Personalized Transformer for Explainable Recommendation”&lt;/strong&gt; 这篇论文，并复现其论文中提及的个性化 Transformer 架构（基于作者的开源代码），并给出代码解读。&lt;/p&gt;">
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2023-07-21">
      <meta 
        property="og:article:modified_time" 
        content="2024-10-28">
      <meta 
        property="og:article:author" 
        content="天泽龟">
      
        
          <meta 
            property="og:article:tag" 
            content="推荐系统">
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
    
    <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.4.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img" 
          src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">天泽龟的龟壳屋</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
      <a 
        href="/bangumis" 
        class="navbar-menu-item">
        
          番剧
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      记人生第一次论文复现以及微小的改进
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2023-07-21T15:58:16.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2023-07-21</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/" 
          class="post-meta-link">
          科研学习
        </a>
      
    
    
      <span class="dot"></span>
      <span>4.2k 字</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" 
            class="post-meta-link">
            推荐系统
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言：</h2>
<p>本篇博客的目的是详细总结 <strong>“Personalized Transformer for Explainable Recommendation”</strong> 这篇论文，并复现其论文中提及的个性化 Transformer 架构（基于作者的开源代码），并给出代码解读。</p>
<span id="more"></span>
<p>除此之外，笔者<s>自以为</s>发现了论文中的一处不足，并尝试对原论文的训练方法进行改进，该尝试最终<s>并没有</s>取得了更好的结果，并据此实验结果，笔者尝试给出了合理解释<s>但没有成功</s>，仅作为笔者熟悉论文过程中的一件轶事。</p>
<br>
<h2 id="论文解读"><a class="markdownIt-Anchor" href="#论文解读"></a> 论文解读：</h2>
<h3 id="1-概论"><a class="markdownIt-Anchor" href="#1-概论"></a> 1. 概论</h3>
<p>该篇论文出自 <code>ACL'21</code>，主要贡献是将推荐系统“个性化”的思想推广至 NLP 大模型领域。</p>
<p>在推荐任务中，用户和产品的 ID 在实现个性化的过程中起到了身份辨识的作用。Transformer 拥有强大的语言建模能力，但它却不够个性化，并且难以利用这些ID， <strong>因为ID和文字完全不在同一个语义空间上</strong> 。过去的工作通常是将 ID token 替换为离散的文本表示（例如提供某些属性词），但离散的特征似乎不能完整地描述某件事物，而且这种特征不能被学习。</p>
<p>为了解决这个问题，作者在可解释推荐这个课题上提出了一种个性化 Transformer（<code>PErsonalized Transformer for Explainable Recommendation</code>，简称 <code>PETER</code> ），用来做基于 ID 的个性化推荐解释任务（即 <strong>“Explanation Generation”</strong>）。</p>
<p>为了赋予 ID 以语言学含义，作者设计了一个简单并且有效的任务，即利用ID来预测需要被生成的解释中的单词（即 <strong>“Context Prediction”</strong>），在二者的语义空间中形成映射关系。</p>
<p>除了生成解释外，借用 <strong>“Rating Prediction”</strong> 的任务，PETER还可以做推荐，从而有效地统一了推荐和解释两大任务。尽管 PETER 模型规模很小并且没有经过预训练，但是在性能和效率上均优于经过微调的BERT，凸显出设计的重要性。</p>
<br>
<h3 id="2-难点"><a class="markdownIt-Anchor" href="#2-难点"></a> 2. 难点</h3>
<p>作者首先尝试将 <code>UserID</code> 与 <code>ItemID</code> 作为一个 Token 对，直接放到 Transformer 里硬 train，但就结果而言是失败的 — — 序列生成完全不会关注这个 Token 对的信息（可以通过 Attention 矩阵观察而得，两个 ID 完全不造成贡献）。</p>
<p><img src="img1.png" alt="img" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="img1.png" class="lozad post-image"></p>
<p>论其原因，作者给出了以下解释：</p>
<blockquote>
<p>试想在某宝某东这样的电商平台，用户数量和产品数量得上亿，但是用户写的点评里也就三千种常用的汉字，这使得 ID 和单词出现的频率非常不匹配（前者频率低，后者频率高），导致模型将 ID 视作词表外的词（OOV token），因此模型对ID完全不敏感。</p>
</blockquote>
<p>因此，作者设计出 “Context Prediction” 这样的任务，即：用产品 ID （用户 ID 亦可）对应的向量来预测需要被生成的单词（一次生成 N 个概率最大的），从而将 Token 向量映射至单词向量对应的语义空间中。</p>
<p>另外，用户可能会主动要求系统解释推荐的产品的某些特征，为了满足这样的需求，作者把这些特征拼接到ID后面来做生成（对应下图的可选位置），记作 PETER+，这样可以生成更有针对性的解释。<br />
<br></p>
<h3 id="3-模型结构"><a class="markdownIt-Anchor" href="#3-模型结构"></a> 3. 模型结构</h3>
<h4 id="31-输入特征"><a class="markdownIt-Anchor" href="#31-输入特征"></a> 3.1 输入特征</h4>
<p>作者在生成序列的开头加入了 User ID 以及 Item ID 这两个 token，并将 ID 与 Word 分别通过 Embedding 层，赋予他们不同的语义空间。对于 PETER+ 模型，还在序列前添加了若干特征 Token 引导生成对应的序列。</p>
<h4 id="32-注意力掩码"><a class="markdownIt-Anchor" href="#32-注意力掩码"></a> 3.2 注意力掩码</h4>
<blockquote>
<p>The <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mrow><mi>t</mi><mi>h</mi></mrow></msub></mrow><annotation encoding="application/x-tex">l_{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> layer encodes the previous layer’s output <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>S</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">S_{l−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>S</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">S_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</p>
</blockquote>
<p>经典的 Transforer 框架采用了一种 “Left-to-Right” 掩码，即每一位只能看到前面的位置。在 PETER 模型中，这做了一些小改动：对于开头的 User ID 和 Item ID，二者可以相互 attend，从而便于做评分预测以及上下文预测的任务。</p>
<p><img src="image-20230723101921864.png" alt="image-20230723101921864" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="image-20230723101921864.png" class="lozad post-image"></p>
<h4 id="33-任务设计"><a class="markdownIt-Anchor" href="#33-任务设计"></a> 3.3 任务设计</h4>
<p>正如开头概论所说，PETER 借由三个任务完成训练。对于不同任务，作者采用了类似的训练策略。</p>
<ul>
<li>解释生成：采用负对数似然（NLL）作为损失函数，并采用贪婪解码的方式生成对应长度的文本。</li>
<li>上下文预测：同样采用负对数似然的方式，将 Item ID 与对应的评论文本作为数据集进行训练，使得可以根据生成序列中 Item 的对应位置，预测出相联系的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{token}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 个词汇。</li>
<li>评分预测：根据两个 tokenID，预测出对应的评分 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mi>r</mi><mo>^</mo></mover><mrow><mi>u</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\hat{r}_{u,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>. 具体做法就是把 UserID 对应位置的矢量丢到 MLP 里硬 train，损失函数选 MSE。</li>
</ul>
<p><img src="v2-39977d62957cf0f3644f937d459bd172_720w.png" alt="v2-39977d62957cf0f3644f937d459bd172_720w" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="v2-39977d62957cf0f3644f937d459bd172_720w.png" class="lozad post-image"></p>
<p><br><br></p>
<h2 id="代码复现"><a class="markdownIt-Anchor" href="#代码复现"></a> 代码复现：</h2>
<p>在尝试自己修改 Transformer 架构复现论文后，笔者发现真正困难的是从零开始造各种轮子。遂求助于论文作者在 Github 上发布的源码，才发现原来已经有各种封装好的类可供使用了。</p>
<br>
<h3 id="1-模型实现"><a class="markdownIt-Anchor" href="#1-模型实现"></a> 1. 模型实现</h3>
<p>不同于 Transformer 的 <code>Encoder-Decoder</code> 架构，PETER 采用的是 <code>Only Encoder</code> 架构，并且该模型未经过预训练，且叠的层数很少，只需要两层便可做到非常好的结果。参考 <code>module.py</code>。</p>
<h4 id="11-encoderlayer-类"><a class="markdownIt-Anchor" href="#11-encoderlayer-类"></a> 1.1 <code>EncoderLayer</code> 类</h4>
<p>用以封装 <code>Encoder</code> 中的单独一层，包含了多头注意力层以及前馈神经网络层，其中多头注意力机制可以直接调库使用。如果参考斯坦福的 Transformer 教程，自己实现多头注意力的话可以封装得更好看（即一层蕴含两个子层），但原理是一样的。</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, nhead, dim_feedforward=<span class="hljs-number">2048</span>, dropout=<span class="hljs-number">0.1</span>, activation=<span class="hljs-string">&quot;relu&quot;</span></span>):<br>        <span class="hljs-built_in">super</span>(EncoderLayer, self).__init__()<br>        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)<br>        <span class="hljs-comment"># Implementation of Feedforward model</span><br>        self.dropout = nn.Dropout(dropout)<br>        self.linear1 = nn.Linear(d_model, dim_feedforward)<br>        self.linear2 = nn.Linear(dim_feedforward, d_model)<br><br>        self.norm1 = nn.LayerNorm(d_model)<br>        self.norm2 = nn.LayerNorm(d_model)<br>        self.dropout1 = nn.Dropout(dropout)<br>        self.dropout2 = nn.Dropout(dropout)<br><br>        self.activation = _get_activation_fn(activation)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__setstate__</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;activation&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> state: state[<span class="hljs-string">&#x27;activation&#x27;</span>] = func.relu<br>        <span class="hljs-built_in">super</span>(EncoderLayer, self).__setstate__(state)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src: Tensor, src_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>, src_key_padding_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, Tensor]:<br><br>        <span class="hljs-comment"># First is multiheadedattention with resiual.</span><br>        src2, attn = self.self_attn(src, src, src, attn_mask=src_mask, <br>                                    key_padding_mask=src_key_padding_mask)<br>        src = self.norm1(src + self.dropout1(src2))<br>        <br>        <span class="hljs-comment"># Second is Feedforward with resiual.</span><br>        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))<br>        src = self.norm2(src + self.dropout2(src2))<br><br>        <span class="hljs-keyword">return</span> src, attn<br><br></code></pre>
<br>
<h3 id="12-encoder-层"><a class="markdownIt-Anchor" href="#12-encoder-层"></a> 1.2  <code>Encoder</code> 层</h3>
<p>将 nlayer 个 <code>EncoderLayer</code> 类对象进行串联。</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEncoder</span>(nn.Module):<br>    __constants__ = [<span class="hljs-string">&#x27;norm&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder_layer, num_layers, norm=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(TransformerEncoder, self).__init__()<br>        self.layers = _get_clones(encoder_layer, num_layers)<br>        self.num_layers = num_layers<br>        self.norm = norm<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src: Tensor, mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>, src_key_padding_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, Tensor]:<br>        output = src<br>        attns = []<br><br>        <span class="hljs-comment"># Append all the layers one by one</span><br>        <span class="hljs-keyword">for</span> mod <span class="hljs-keyword">in</span> self.layers:<br>            output, attn = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)<br>            attns.append(attn)<br>        attns = torch.stack(attns)<br><br>        <span class="hljs-keyword">if</span> self.norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            output = self.norm(output)<br><br>        <span class="hljs-keyword">return</span> output, attns<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_clones</span>(<span class="hljs-params">module, N</span>):<br>    <span class="hljs-comment"># Create N deepcopy of module</span><br>    <span class="hljs-keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)])<br></code></pre>
<br>
<h4 id="13-预测评分的-mlp-层"><a class="markdownIt-Anchor" href="#13-预测评分的-mlp-层"></a> 1.3 预测评分的 MLP 层</h4>
<p>激活函数选用 Sigmoid 函数，比较典型，略去不谈。</p>
<br>
<h4 id="14-peter-模型类"><a class="markdownIt-Anchor" href="#14-peter-模型类"></a> 1.4 PETER 模型类</h4>
<p>将输入、<code>Eocoder</code> 模型以及输出部分整合起来的完整版。三个任务的训练函数、注意力掩码也在该类中实现。</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PETER</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, peter_mask, src_len, tgt_len, pad_idx, nuser, nitem, ntoken, emsize, nhead, nhid, nlayers, dropout=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>(PETER, self).__init__()<br>        <br>        <span class="hljs-comment"># emsize: word embedding size</span><br>        self.pos_encoder = PositionalEncoding(emsize, dropout)  <br>        <span class="hljs-comment"># nhid: dim_feedforward, one basic layer, including multi-head attention and FFN</span><br>        encoder_layers = EncoderLayer(emsize, nhead, nhid, dropout)  <br>        <span class="hljs-comment"># loop over the one above</span><br>        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)  <br>        <br>        <span class="hljs-comment"># Embedding ID &amp; words with different param.s</span><br>        self.user_embeddings = nn.Embedding(nuser, emsize)<br>        self.item_embeddings = nn.Embedding(nitem, emsize)<br>        self.word_embeddings = nn.Embedding(ntoken, emsize)<br>        <br>        <span class="hljs-comment"># For generation/prediciton tasks</span><br>        self.hidden2token = nn.Linear(emsize, ntoken)<br>        self.recommender = MLP(emsize)<br>        self.ui_len = <span class="hljs-number">2</span> <span class="hljs-comment"># UserID &amp; ItemID</span><br>        self.src_len = src_len <span class="hljs-comment"># Including feature tokens</span><br>        self.pad_idx = pad_idx<br>        self.emsize = emsize<br>        <span class="hljs-keyword">if</span> peter_mask:<br>            self.attn_mask = generate_peter_mask(src_len, tgt_len)<br>        <span class="hljs-keyword">else</span>:<br>            self.attn_mask = generate_square_subsequent_mask(src_len + tgt_len)<br><br>        self.init_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">self</span>):<br>        initrange = <span class="hljs-number">0.1</span><br>        self.user_embeddings.weight.data.uniform_(-initrange, initrange)<br>        self.item_embeddings.weight.data.uniform_(-initrange, initrange)<br>        self.word_embeddings.weight.data.uniform_(-initrange, initrange)<br>        self.hidden2token.weight.data.uniform_(-initrange, initrange)<br>        self.hidden2token.bias.data.zero_()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_context</span>(<span class="hljs-params">self, hidden</span>):<br>        <span class="hljs-comment"># 下标为 1 表示 Item ID</span><br>        context_prob = self.hidden2token(hidden[<span class="hljs-number">1</span>])  <span class="hljs-comment"># (batch_size, ntoken)</span><br>        log_context_dis = func.log_softmax(context_prob, dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> log_context_dis<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_rating</span>(<span class="hljs-params">self, hidden</span>):<br>        <span class="hljs-comment"># 下标为 0 表示 User ID</span><br>        rating = self.recommender(hidden[<span class="hljs-number">0</span>])  <span class="hljs-comment"># (batch_size,)</span><br>        <span class="hljs-keyword">return</span> rating<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_seq</span>(<span class="hljs-params">self, hidden</span>):<br>        <span class="hljs-comment"># 根据评论序列（从 src_len 起）预测评论</span><br>        word_prob = self.hidden2token(hidden[self.src_len:])  <span class="hljs-comment"># (tgt_len, batch_size, ntoken)</span><br>        log_word_prob = func.log_softmax(word_prob, dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> log_word_prob<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_token</span>(<span class="hljs-params">self, hidden</span>):<br>        word_prob = self.hidden2token(hidden[-<span class="hljs-number">1</span>])  <span class="hljs-comment"># (batch_size, ntoken)</span><br>        log_word_prob = func.log_softmax(word_prob, dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> log_word_prob<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_square_subsequent_mask</span>(<span class="hljs-params">total_len</span>):<br>        mask = torch.tril(torch.ones(total_len, total_len))  <br>        <span class="hljs-comment"># (total_len, total_len), lower triangle -&gt; 1.; others 0.</span><br>        mask = mask == <span class="hljs-number">0</span>  <span class="hljs-comment"># lower -&gt; False; others True</span><br>        <span class="hljs-keyword">return</span> mask<br><br>	<span class="hljs-comment"># 生成对应的 PETER 掩码</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_peter_mask</span>(<span class="hljs-params">src_len, tgt_len</span>):<br>        total_len = src_len + tgt_len<br>        mask = generate_square_subsequent_mask(total_len)<br>        mask[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] = <span class="hljs-literal">False</span>  <span class="hljs-comment"># allow to attend for user and item</span><br>        <span class="hljs-keyword">return</span> mask<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, user, item, text, seq_prediction=<span class="hljs-literal">True</span>, </span><br><span class="hljs-params">                context_prediction=<span class="hljs-literal">True</span>, rating_prediction=<span class="hljs-literal">True</span></span>):<br>        device = user.device<br>        batch_size = user.size(<span class="hljs-number">0</span>)<br>        total_len = self.ui_len + text.size(<span class="hljs-number">0</span>)  <br>        <br>        <span class="hljs-comment"># 使用 attn_mask 及 key_padding_mask 做注意力掩码</span><br>        attn_mask = self.attn_mask[:total_len, :total_len].to(device)  <span class="hljs-comment"># (total_len, total_len)</span><br>        left = torch.zeros(batch_size, self.ui_len).<span class="hljs-built_in">bool</span>().to(device)  <span class="hljs-comment"># (batch_size, ui_len)</span><br>        right = text.t() == self.pad_idx  <br>        <span class="hljs-comment"># replace pad_idx with True and others with False, (batch_size, total_len - ui_len)</span><br>        key_padding_mask = torch.cat([left, right], <span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, total_len)</span><br><br>        <span class="hljs-comment"># 对三个不同语义下的输入做 Embedding</span><br>        u_src = self.user_embeddings(user.unsqueeze(<span class="hljs-number">0</span>))  <span class="hljs-comment"># (1, batch_size, emsize)</span><br>        i_src = self.item_embeddings(item.unsqueeze(<span class="hljs-number">0</span>))  <span class="hljs-comment"># (1, batch_size, emsize)</span><br>        w_src = self.word_embeddings(text)  <span class="hljs-comment"># (total_len - ui_len, batch_size, emsize)</span><br>        src = torch.cat([u_src, i_src, w_src], <span class="hljs-number">0</span>)  <span class="hljs-comment"># (total_len, batch_size, emsize)</span><br>        src = src * math.sqrt(self.emsize)<br>        <br>        <span class="hljs-comment"># 加上位置嵌入后开 train</span><br>        src = self.pos_encoder(src)<br>        hidden, attns = self.transformer_encoder(src, attn_mask, key_padding_mask)  <br>        <span class="hljs-comment"># (total_len, batch_size, emsize) vs. (nlayers, batch_size, total_len_tgt, total_len_src)</span><br>        <br>        <span class="hljs-comment"># 做三个训练任务</span><br>        <span class="hljs-keyword">if</span> rating_prediction:<br>            rating = self.predict_rating(hidden)  <span class="hljs-comment"># (batch_size,)</span><br>        <span class="hljs-keyword">else</span>:<br>            rating = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> context_prediction:<br>            log_context_dis = self.predict_context(hidden)  <span class="hljs-comment"># (batch_size, ntoken)</span><br>        <span class="hljs-keyword">else</span>:<br>            log_context_dis = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> seq_prediction:<br>            log_word_prob = self.predict_seq(hidden)  <span class="hljs-comment"># (tgt_len, batch_size, ntoken)</span><br>        <span class="hljs-keyword">else</span>:<br>            log_word_prob = self.generate_token(hidden)  <span class="hljs-comment"># (batch_size, ntoken)</span><br>        <br>        <span class="hljs-comment"># 返回三个任务的推理结果，以及注意力矩阵</span><br>        <span class="hljs-keyword">return</span> log_word_prob, log_context_dis, rating, attns<br></code></pre>
<br>
<h3 id="2-训练过程"><a class="markdownIt-Anchor" href="#2-训练过程"></a> 2. 训练过程</h3>
<p>包含了读取数据及批处理、模型构建及训练、以及最后的评估工作。参考 <code>main.py</code>。</p>
<br>
<h4 id="21-读取数据"><a class="markdownIt-Anchor" href="#21-读取数据"></a> 2.1 读取数据</h4>
<p>笔者这里选用的数据集是 <code>TripAdvisor</code>，收集自香港各大酒店的用户反馈，包含 <code>UserID, ItemID, features, words</code> 等信息。读取数据时就做做词嵌入、数据预处理（例如，加入 <code>&lt;bos&gt;</code> 与 <code>&lt;eos&gt;</code> 标签，输入长度的截取与补全）以及批处理。</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-built_in">print</span>(now_time() + <span class="hljs-string">&#x27;Loading data&#x27;</span>)<br>corpus = DataLoader(args.data_path, args.index_dir, args.vocab_size)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;text: &quot;</span> + corpus.train[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;text&#x27;</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;filtered text: &quot;</span> + corpus.train[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;filter&#x27;</span>])<br><br>word2idx = corpus.word_dict.word2idx<br>idx2word = corpus.word_dict.idx2word<br>feature_set = corpus.feature_set<br>train_data = Batchify(corpus.train, word2idx, args.words, args.batch_size, shuffle=<span class="hljs-literal">True</span>)<br>val_data = Batchify(corpus.valid, word2idx, args.words, args.batch_size)<br>test_data = Batchify(corpus.test, word2idx, args.words, args.batch_size)<br></code></pre>
<br>
<h4 id="22-模型构建"><a class="markdownIt-Anchor" href="#22-模型构建"></a> 2.2 模型构建</h4>
<p>敲定各种参数，以及选用对应的优化器（居然是 SGD 而非 Adam）以及损失函数。</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-comment"># 各种参数</span><br>tgt_len = args.words + <span class="hljs-number">1</span>  <span class="hljs-comment"># added &lt;bos&gt; or &lt;eos&gt;</span><br>ntokens = <span class="hljs-built_in">len</span>(corpus.word_dict)<br>nuser   = <span class="hljs-built_in">len</span>(corpus.user_dict)<br>nitem   = <span class="hljs-built_in">len</span>(corpus.item_dict)<br>pad_idx = word2idx[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>] <span class="hljs-comment"># 空白占位符</span><br><br><span class="hljs-comment"># 读取模型</span><br>model = PETER(args.peter_mask, src_len, tgt_len, pad_idx, nuser, nitem, ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)<br><br><span class="hljs-comment"># 损失函数以及优化器</span><br>text_criterion = nn.NLLLoss(ignore_index=pad_idx)  <span class="hljs-comment"># ignore the padding when computing loss</span><br>rating_criterion = nn.MSELoss()<br>optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)<br>scheduler = torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.25</span>)<br></code></pre>
<br>
<h4 id="23-训练过程"><a class="markdownIt-Anchor" href="#23-训练过程"></a> 2.3 训练过程</h4>
<pre class="highlight"><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">data</span>):<br>    <span class="hljs-comment"># Turn on training mode which enables dropout.</span><br>    model.train()<br>    context_loss = <span class="hljs-number">0.</span><br>    text_loss = <span class="hljs-number">0.</span><br>    rating_loss = <span class="hljs-number">0.</span><br>    total_sample = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># (batch_size, seq_len), data.step += 1</span><br>        user, item, rating, seq, filt, feature = data.next_batch()  <br>        batch_size = user.size(<span class="hljs-number">0</span>)<br>        user = user.to(device)  <span class="hljs-comment"># (batch_size,)</span><br>        item = item.to(device)<br>        rating = rating.to(device)<br><br>        seq = seq.t().to(device)  <span class="hljs-comment"># (tgt_len + 1, batch_size)</span><br>        feature = feature.t().to(device)  <span class="hljs-comment"># (1, batch_size)</span><br>        <span class="hljs-keyword">if</span> args.use_feature:<br>            text = torch.cat([feature, seq[:-<span class="hljs-number">1</span>]], <span class="hljs-number">0</span>)  <span class="hljs-comment"># (src_len + tgt_len - 2, batch_size)</span><br>        <span class="hljs-keyword">else</span>:<br>            text = seq[:-<span class="hljs-number">1</span>]  <span class="hljs-comment"># (src_len + tgt_len - 2, batch_size)</span><br>        <span class="hljs-comment"># Starting each batch, we detach the hidden state from how it was previously produced.</span><br>        <span class="hljs-comment"># If we didn&#x27;t, the model would try backpropagating all the way to start of the dataset.</span><br>        optimizer.zero_grad()<br><br>        log_word_prob, log_context_dis, rating_p, _ = model(user, item, text)  <span class="hljs-comment"># (tgt_len, batch_size, ntoken) vs. (batch_size, ntoken) vs. (batch_size,)</span><br>        context_dis = log_context_dis.unsqueeze(<span class="hljs-number">0</span>).repeat((tgt_len - <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># (batch_size, ntoken) -&gt; (tgt_len - 1, batch_size, ntoken)</span><br>        <br>        c_loss = text_criterion(context_dis.view(-<span class="hljs-number">1</span>, ntokens), seq[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>].reshape((-<span class="hljs-number">1</span>,)))<br>        r_loss = rating_criterion(rating_p, rating)<br>        t_loss = text_criterion(log_word_prob.view(-<span class="hljs-number">1</span>, ntokens), seq[<span class="hljs-number">1</span>:].reshape((-<span class="hljs-number">1</span>,)))<br>        loss = args.text_reg * t_loss + args.context_reg * c_loss + args.rating_reg * r_loss<br>        loss.backward()<br><br>        <span class="hljs-comment"># `clip_grad_norm` helps prevent the exploding gradient problem.</span><br>        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)<br>        optimizer.step()<br><br>        context_loss += batch_size * c_loss.item()<br>        text_loss += batch_size * t_loss.item()<br>        rating_loss += batch_size * r_loss.item()<br>        total_sample += batch_size<br>        <br>        <span class="hljs-keyword">if</span> data.step == data.total_step:<br>            <span class="hljs-keyword">break</span><br></code></pre>
<p><br><br></p>
<h2 id="改进尝试"><a class="markdownIt-Anchor" href="#改进尝试"></a> 改进尝试：</h2>
<h3 id="1-思想简述"><a class="markdownIt-Anchor" href="#1-思想简述"></a> 1. 思想简述</h3>
<p>笔者在阅读论文的过程中，注意到论文作者给出的推理样例如下：<br />
<img src="table1.png" alt="image-20230723161838532" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="table1.png" class="lozad post-image"></p>
<br>
<p>从样例中可以看出，PETER 模型确实可以通过生成 Context 与 Explanation，为 <code>User-Item</code> 的 Token 对赋予语义学信息，并能根据 ID 信息实现一定程度的个性化（参考样例中的下划线特征）。</p>
<p>然而笔者发现，在 “Context Prediction” 任务中，大量的预测词汇不具有实际含义，例如 the, and, a…… 这些词频繁地出现在语料中，才以较高的概率成为被预测的上下文。但是这种预测显然不符合 “赋予 ID 以语义学信息” 的任务目的。</p>
<p>作者显然也意识到了这个问题，他据此提出 PETER+ ，通过引入特征，指导模型的生成任务。作者在论文中指出：</p>
<blockquote>
<p>Admittedly, there is still <strong>much room for improvement</strong> of the context prediction task, so as to more accurately predict the features in the ground-truth (e.g., rooms vs. pool in the first example).</p>
</blockquote>
<p>但笔者认为可以通过一个更加简单粗暴的做法解决该问题：既然上下文的预测结果大都是频繁出现在语料中而无实意的词汇，那么只需对语料预处理的时候进行<strong>停用词筛选</strong>，并且在训练的过程中采用非停用词序列去优化上下文预测任务，便可以较好地规避停用词所造成的影响。</p>
<br>
<h3 id="2-代码修改"><a class="markdownIt-Anchor" href="#2-代码修改"></a> 2. 代码修改</h3>
<p>为了改进语料预处理流程，在其中加入停用词筛选这一步骤，我们需要对论文源代码中的 <code>util.py</code> 模块进行修改。我们先引入 nltk 包，并从中导入常见停用词列表：</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-comment"># 加载并导入停用词列表</span><br>nltk.download(<span class="hljs-string">&#x27;stopwords&#x27;</span>)<br>stop_words = <span class="hljs-built_in">set</span>(nltk.corpus.stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>))<br><span class="hljs-comment"># &gt;&gt; &#123;&#x27;between&#x27;, &#x27;does&#x27;, &#x27;because&#x27;, &#x27;ll&#x27;, &#x27;on&#x27;, &#x27;all&#x27;, ...&#125;</span><br></code></pre>
<br>
<p>在 <code>util.py</code>模块中，我们找到加载数据集对应的方法如下：</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-comment"># 将原评论序列中的单词转为词嵌入表示</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq2ids</span>(<span class="hljs-params">self, seq</span>):<br>    <span class="hljs-keyword">return</span> [self.word_dict.word2idx.get(w, self.__unk) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq.split()]<br><br><span class="hljs-keyword">for</span> review <span class="hljs-keyword">in</span> reviews:<br>    (fea, adj, tem, sco) = review[<span class="hljs-string">&#x27;template&#x27;</span>]<br>    data.append(&#123;<span class="hljs-string">&#x27;user&#x27;</span>: self.user_dict.entity2idx[review[<span class="hljs-string">&#x27;user&#x27;</span>]],<br>                 <span class="hljs-string">&#x27;item&#x27;</span>: self.item_dict.entity2idx[review[<span class="hljs-string">&#x27;item&#x27;</span>]],<br>                 <span class="hljs-string">&#x27;rating&#x27;</span>: review[<span class="hljs-string">&#x27;rating&#x27;</span>],<br>                 <span class="hljs-comment"># &#x27;text&#x27; 属性对应词嵌入后表示的矢量序列</span><br>                 <span class="hljs-string">&#x27;text&#x27;</span>: self.seq2ids(tem),<br>                 <span class="hljs-string">&#x27;feature&#x27;</span>: self.word_dict.word2idx.get(fea, self.__unk)&#125;)<br>    <span class="hljs-keyword">if</span> fea <span class="hljs-keyword">in</span> self.word_dict.word2idx:<br>        self.feature_set.add(fea)<br>    <span class="hljs-keyword">else</span>:<br>        self.feature_set.add(<span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>)<br></code></pre>
<br>
<p>现在我们尝试向数据集中添加一个 <code>filtered_text</code> 属性，表示经过停用词筛选后的评论文本序列。代码修改如下：</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq2filtered_ids</span>(<span class="hljs-params">self, seq</span>):<br><span class="hljs-comment"># 仅当 w 不在停用词列表中，才将其加入序列</span><br>    <span class="hljs-keyword">return</span> [self.word_dict.word2idx.get(w, self.__unk) <br>            <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq.split() <span class="hljs-keyword">if</span> w <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words]<br><br><span class="hljs-comment"># ... 前后代码无变化</span><br><span class="hljs-string">&#x27;text&#x27;</span>: self.seq2ids(tem),<br><span class="hljs-string">&#x27;filtered_text&#x27;</span>: self.seq2filtered_ids(tem),<br><span class="hljs-comment"># ... 前后代码无变化</span><br></code></pre>
<br>
<p>在数据批处理的过程中，<code>filtered_text</code>字段必定小于等于原<code>text</code>字段，长度不足的地方我们用 <code>&lt;pad&gt;</code> 进行补齐。由于我们仅用该字段处理上下文预测任务的损失函数优化过程，因此 <code>&lt;bos&gt;</code> 和 <code>&lt;eos&gt;</code> 也非必须存在，可以为 <code>&lt;pad&gt;</code> 替代。</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-comment"># Batchify 类用来对数据进行批处理</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Batchify</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data, word2idx, seq_len=<span class="hljs-number">15</span>, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">False</span></span>):<br>        bos = word2idx[<span class="hljs-string">&#x27;&lt;bos&gt;&#x27;</span>]<br>        eos = word2idx[<span class="hljs-string">&#x27;&lt;eos&gt;&#x27;</span>]<br>        pad = word2idx[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>]<br>        u, i, r, t, tf, f = [], [], [], [], [], []<br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> data:<br>            u.append(x[<span class="hljs-string">&#x27;user&#x27;</span>])<br>            i.append(x[<span class="hljs-string">&#x27;item&#x27;</span>])<br>            r.append(x[<span class="hljs-string">&#x27;rating&#x27;</span>])<br>            <span class="hljs-comment"># 训练文本序列应具有相同长度，因此需要截断或者用&lt;pad&gt;填充</span><br>            t.append(sentence_format(x[<span class="hljs-string">&#x27;text&#x27;</span>], seq_len, pad, bos, eos))<br>            <span class="hljs-comment">#筛选文本序列仅用来优化上下文预测任务，只保留关键语义信息</span><br>            tf.append(sentence_format(x[<span class="hljs-string">&#x27;filtered_text&#x27;</span>], seq_len, pad, pad, pad))<br>            f.append([x[<span class="hljs-string">&#x27;feature&#x27;</span>]])   <br></code></pre>
<br>
<p>在 <code>main.py</code> 中，我们只需将 “上下文预测” 任务代码中参照的原序列（seq），修改为经过筛选后的序列（filtered_seq）即可：</p>
<pre class="highlight"><code class="hljs py"><span class="hljs-comment"># 此处用来训练的语料为筛选后的序列</span><br>c_loss = text_criterion(context_dis.view(-<span class="hljs-number">1</span>, ntokens), filtered_seq[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>].reshape((-<span class="hljs-number">1</span>,)))<br></code></pre>
<br>
<h3 id="3-实验结果"><a class="markdownIt-Anchor" href="#3-实验结果"></a> 3. 实验结果</h3>
<p>笔者将改进后的模型记为 PETERF（虽然模型结构本身没有改变，只是在训练方法上做了调整），并将其与原模型进行对比。各项指标对比如下：</p>
<p><img src="table2.png" alt="image-20230724223205321" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="table2.png" class="lozad post-image"></p>
<br>
<p>如图可知，绝大多数指标有所提升，说明笔者的改进方法确实有利于模型训练。<s>但笔者无法解释各项指标所对应的含义，所以也无法向读者说明改进方法具体从哪些方向改善了模型。</s></p>
<p>此外，尽管各指标均有所提升，但提升幅度极小，所以这项改进工作也可忽略不计。<s>但这项任务本来就是笔者灵光乍现所尝试的，不确定是否具有实际意义。</s></p>

  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            天泽龟
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://tzturtle.moe/2023/07/21/Personalized_Transformer/">
            https://tzturtle.moe/2023/07/21/Personalized_Transformer/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
    <div class="nav-item-prev">
      <a 
        href="/2023/07/24/Computer-Organization-and-Architecture-CPU/" 
        class="nav-link">
        <i class="iconfont icon-left nav-prev-icon"></i>
        <div>
          <div class="nav-label">上一篇</div>
          
            <div class="nav-title">计算机组成原理：CPU 与流水线技术 </div>
          
        </div>
      </a>
    </div>
  
  
    <div class="nav-item-next">
      <a 
        href="/2023/07/04/LLM-learning/" 
        class="nav-link">
        <div>
          <div class="nav-label">下一篇</div>
          
            <div class="nav-title">大模型相关论文阅读笔记 </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: '4CiiPBXpbjDnPvIIwfuEPEY6-gzGzoHsz',
        appKey: '3AQY35K3Laq9fLvTG2uOHDUT',
        placeholder: '留下你的评论...',
        path: window.location.pathname,
        avatar: 'identicon',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'false',
        highlight: true,
        recordIP: false,
        
        
        
        enableQQ: 'true',
        requiredFields: [],
      });
    };
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text"> 前言：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB"><span class="toc-text"> 论文解读：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E8%AE%BA"><span class="toc-text"> 1. 概论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%9A%BE%E7%82%B9"><span class="toc-text"> 2. 难点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-text"> 3. 模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#31-%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81"><span class="toc-text"> 3.1 输入特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#32-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A9%E7%A0%81"><span class="toc-text"> 3.2 注意力掩码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#33-%E4%BB%BB%E5%8A%A1%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 3.3 任务设计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0"><span class="toc-text"> 代码复现：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 1. 模型实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-encoderlayer-%E7%B1%BB"><span class="toc-text"> 1.1 EncoderLayer 类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-encoder-%E5%B1%82"><span class="toc-text"> 1.2  Encoder 层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#13-%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86%E7%9A%84-mlp-%E5%B1%82"><span class="toc-text"> 1.3 预测评分的 MLP 层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-peter-%E6%A8%A1%E5%9E%8B%E7%B1%BB"><span class="toc-text"> 1.4 PETER 模型类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text"> 2. 训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#21-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text"> 2.1 读取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#22-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-text"> 2.2 模型构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text"> 2.3 训练过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E5%B0%9D%E8%AF%95"><span class="toc-text"> 改进尝试：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%80%9D%E6%83%B3%E7%AE%80%E8%BF%B0"><span class="toc-text"> 1. 思想简述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9"><span class="toc-text"> 2. 代码修改</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text"> 3. 实验结果</span></a></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
    class="author-img" 
    alt="author avatar">

<p class="author-name">天泽龟</p>
<p class="author-description">天泽龟的龟是龟裂的龟哦。</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>61</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>5</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>15</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://space.bilibili.com/12645985">
          <i class="iconfont icon-bilibili society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://github.com/TURLEing">
          <i class="iconfont icon-github society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://turleing.github.io/about/">
          <i class="iconfont icon-mail society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text"> 前言：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB"><span class="toc-text"> 论文解读：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E8%AE%BA"><span class="toc-text"> 1. 概论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%9A%BE%E7%82%B9"><span class="toc-text"> 2. 难点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-text"> 3. 模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#31-%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81"><span class="toc-text"> 3.1 输入特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#32-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A9%E7%A0%81"><span class="toc-text"> 3.2 注意力掩码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#33-%E4%BB%BB%E5%8A%A1%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 3.3 任务设计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0"><span class="toc-text"> 代码复现：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 1. 模型实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-encoderlayer-%E7%B1%BB"><span class="toc-text"> 1.1 EncoderLayer 类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-encoder-%E5%B1%82"><span class="toc-text"> 1.2  Encoder 层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#13-%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86%E7%9A%84-mlp-%E5%B1%82"><span class="toc-text"> 1.3 预测评分的 MLP 层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-peter-%E6%A8%A1%E5%9E%8B%E7%B1%BB"><span class="toc-text"> 1.4 PETER 模型类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text"> 2. 训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#21-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text"> 2.1 读取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#22-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-text"> 2.2 模型构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text"> 2.3 训练过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E5%B0%9D%E8%AF%95"><span class="toc-text"> 改进尝试：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%80%9D%E6%83%B3%E7%AE%80%E8%BF%B0"><span class="toc-text"> 1. 思想简述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9"><span class="toc-text"> 2. 代码修改</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text"> 3. 实验结果</span></a></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/%E7%94%9F%E6%B4%BB%E5%88%86%E4%BA%AB/">
        <div class="categories-list-item">
          生活分享
          <span class="categories-list-item-badge">10</span>
        </div>
      </a>
    
      <a href="/categories/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/">
        <div class="categories-list-item">
          算法竞赛
          <span class="categories-list-item-badge">12</span>
        </div>
      </a>
    
      <a href="/categories/%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          专业学习
          <span class="categories-list-item-badge">27</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          科研学习
          <span class="categories-list-item-badge">8</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A4%BE%E5%9B%A2%E6%8E%A8%E9%80%81/">
        <div class="categories-list-item">
          社团推送
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" 
        title="计算机组成原理">
        <div class="tags-list-item">计算机组成原理</div>
      </a>
    
      <a 
        href="/tags/%E9%9A%8F%E7%AC%94/" 
        title="随笔">
        <div class="tags-list-item">随笔</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" 
        title="强化学习">
        <div class="tags-list-item">强化学习</div>
      </a>
    
      <a 
        href="/tags/NLP/" 
        title="NLP">
        <div class="tags-list-item">NLP</div>
      </a>
    
      <a 
        href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" 
        title="操作系统">
        <div class="tags-list-item">操作系统</div>
      </a>
    
      <a 
        href="/tags/%E6%97%A5%E9%BA%BB/" 
        title="日麻">
        <div class="tags-list-item">日麻</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" 
        title="大模型">
        <div class="tags-list-item">大模型</div>
      </a>
    
      <a 
        href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" 
        title="字符串">
        <div class="tags-list-item">字符串</div>
      </a>
    
      <a 
        href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" 
        title="分布式">
        <div class="tags-list-item">分布式</div>
      </a>
    
      <a 
        href="/tags/%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97/" 
        title="情感计算">
        <div class="tags-list-item">情感计算</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F/" 
        title="多项式">
        <div class="tags-list-item">多项式</div>
      </a>
    
      <a 
        href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" 
        title="推荐系统">
        <div class="tags-list-item">推荐系统</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" 
        title="强连通分量">
        <div class="tags-list-item">强连通分量</div>
      </a>
    
      <a 
        href="/tags/%E7%BA%BF%E6%80%A7%E5%9F%BA/" 
        title="线性基">
        <div class="tags-list-item">线性基</div>
      </a>
    
      <a 
        href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" 
        title="博弈论">
        <div class="tags-list-item">博弈论</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text"> 前言：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB"><span class="toc-text"> 论文解读：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A6%82%E8%AE%BA"><span class="toc-text"> 1. 概论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%9A%BE%E7%82%B9"><span class="toc-text"> 2. 难点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-text"> 3. 模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#31-%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81"><span class="toc-text"> 3.1 输入特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#32-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A9%E7%A0%81"><span class="toc-text"> 3.2 注意力掩码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#33-%E4%BB%BB%E5%8A%A1%E8%AE%BE%E8%AE%A1"><span class="toc-text"> 3.3 任务设计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0"><span class="toc-text"> 代码复现：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text"> 1. 模型实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-encoderlayer-%E7%B1%BB"><span class="toc-text"> 1.1 EncoderLayer 类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-encoder-%E5%B1%82"><span class="toc-text"> 1.2  Encoder 层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#13-%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86%E7%9A%84-mlp-%E5%B1%82"><span class="toc-text"> 1.3 预测评分的 MLP 层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-peter-%E6%A8%A1%E5%9E%8B%E7%B1%BB"><span class="toc-text"> 1.4 PETER 模型类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text"> 2. 训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#21-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text"> 2.1 读取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#22-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-text"> 2.2 模型构建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text"> 2.3 训练过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E5%B0%9D%E8%AF%95"><span class="toc-text"> 改进尝试：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%80%9D%E6%83%B3%E7%AE%80%E8%BF%B0"><span class="toc-text"> 1. 思想简述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9"><span class="toc-text"> 2. 代码修改</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text"> 3. 实验结果</span></a></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-09-28</div>
        <a href="/2025/09/28/CS336/"><div class="recent-posts-item-content">CS336 学习笔记 Part 1：从模型架构变体到底层硬件优化</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-08-15</div>
        <a href="/2025/08/15/my-interest-in-ag/"><div class="recent-posts-item-content">现代大模型时代下的情感计算综述</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-07-21</div>
        <a href="/2025/07/21/Rl-reproduce/"><div class="recent-posts-item-content">RL 实验复现随笔【Tool Agent】【PPO、GRPO】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-07-16</div>
        <a href="/2025/07/16/daily-story-pyenv-conda/"><div class="recent-posts-item-content">【实验室小品一则】什么是pyenv，什么是miniconda</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020 -
          
          2025
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          天泽龟的龟壳屋
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer> 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton" 
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
    
      <script> 
        loadScript('/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
    
  </body>
</html>
