<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="天泽龟的龟壳屋">
  <link 
    rel="icon" 
    href="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
  <title>benchmark-for-llmagent</title><meta name="robots" content="noindex">
  
    
      <meta 
        property="og:title" 
        content="benchmark-for-llmagent">
    
    
      <meta 
        property="og:url" 
        content="https://tzturtle.moe/2025/08/19/benchmark-for-llmagent/index.html">
    
    
      <meta 
        property="og:img" 
        content="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg">
    
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2025-08-19">
      <meta 
        property="og:article:modified_time" 
        content="2025-08-26">
      <meta 
        property="og:article:author" 
        content="天泽龟">
      
        
      
    
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
      }
    };
    setDarkmode();
    </script>
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
    
    <link rel="prefetch" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" as="script">
  
  
    
    <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
<meta name="generator" content="Hexo 5.4.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
        <img 
          class="navbar-logo-img" 
          src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">天泽龟的龟壳屋</span>
    </span>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/about" 
        class="navbar-menu-item">
        
          关于
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
      <a 
        href="/bangumis" 
        class="navbar-menu-item">
        
          番剧
        
      </a>
    
    <a 
      class="navbar-menu-item darknavbar" 
      id="dark">
      <i class="iconfont icon-weather"></i>
    </a>
    <a 
      class="navbar-menu-item searchnavbar" 
      id="search">
      <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i>
    </a>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      benchmark-for-llmagent
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2025-08-19T12:06:50.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2025-08-19</span>
    </time>
    
    
      <span class="dot"></span>
      <span>5.5k 字</span>
    
  </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <h2 id="调研目标"><a class="markdownIt-Anchor" href="#调研目标"></a> 调研目标</h2>
<p>将 Benchmark 视作需求，弄清楚如何从需求中搭建环境，如何设计 Reward 等等……算法是重要的，但 researcher is engineer，搞明白训练框架 / RL 全栈也很重要。</p>
<p><img src="GrkSFvsboAE8_e9.png" alt="A benchmarking table titled Claude 4 benchmarks comparing performance metrics across various capabilities including coding, reasoning, tool use, multilingual Q&amp;A, visual reasoning, and mathematics." / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="GrkSFvsboAE8_e9.png" class="lozad post-image"></p>
<h2 id="τ-bench"><a class="markdownIt-Anchor" href="#τ-bench"></a> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">τ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.1132em;">τ</span></span></span></span>-Bench</h2>
<ul>
<li><strong>定位</strong> ：通过模拟用户与智能体的对话，基于 “Database”，“Function Calling” 以及 “Policy” 的两类场景（Retail &amp; Airport）下的任务，测试模型调用工具，和人类交互，解决实际问题的能力。</li>
<li><strong>优势</strong> ：1）确实够痛点，而且切中大模型发展的 key need；2）但另一方面，with advance of LMs，也是发展的 nature need；3）搭建场景（env），构造 entities（LM），对 Benchmark 的数据 verify ，这套构筑流程很自然；</li>
<li><strong>评估</strong> ：基于最终数据库与预期结果的一致性以及智能体对用户响应的信息完整性计算（output-based）；提出了一个 <code>pass^k</code> 的指标，用以衡量一次任务重复 k 次的一致性。</li>
<li><strong>缺点</strong> ：1）场景太少，not general；2）用户模拟不够真实；</li>
</ul>
<h2 id="τ2-bench"><a class="markdownIt-Anchor" href="#τ2-bench"></a> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>τ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">τ^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.1132em;">τ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Bench</h2>
<ul>
<li><strong>定位</strong> ：主要是解决第二个缺点，通过设计一类新的动作（Telecom）允许用户和自己的环境/工具交互。这种“双控”模式使得评估更加真实，因为它测试了 Agent 在需要与用户协作时的沟通和协调能力。</li>
<li><strong>优势</strong> ：比起上一篇论文，还做了一些优化。1）在构造数据集的时候，先构造一批原子的 subtask，通过将不同的子任务组合连接，构造复杂的复合任务；这样的好处是更容易验证结果是否可以唯一由任务导致。2）用户也可以操作自己的环境，需要和自己的环境交互，相较于完全由 agent 操作，会造成 performance 下降（但感觉是用户的问题；</li>
<li><strong>缺点</strong> ：我觉得最大的问题一直是领域不 general，扩展基准测试的覆盖范围（即创建新的业务领域）仍然依赖于人类专家参与。为了让这类基准测试方法被行业广泛采纳，未来需要进一步研究如何自动化领域的创建过程。</li>
</ul>
<h2 id="swe-bench"><a class="markdownIt-Anchor" href="#swe-bench"></a> SWE-Bench</h2>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/16292266518">SWE-Bench：如何构造 LLM 时代的优秀 Benchmark - 知乎</a></p>
<ul>
<li>
<p><strong>定位</strong> ：解决现实世界 Github Issues。</p>
</li>
<li>
<p><strong>优势</strong> ：1）<strong>贵在真实</strong>，真的是从 github Issue 直接掏出来鲜活的数据，非常有价值；2）<strong>难</strong>，一个 Benchmark 如果在放出来的时候就能让人刷到80多90多，那没有人是愿意去刷这个榜的。3）<strong>易于评估</strong>，这是我觉得最重要的，如果把 benchmark 视为产品，第一重要的是产品曝光度，第二是要让用户好上手。</p>
</li>
</ul>
<blockquote>
<p>假设要在 25 年的上半年发布一个很难的 benchmark，应该做出预期，让 25 年年中的 SOTA model 的水平只能得到 10 分以下，而现在的 SOTA model 只能得到 1 分左右。否则很有可能 benchmark 发布的一瞬间就被那时候的 SOTA model 秒了，也即 die at release。</p>
</blockquote>
<p>面向 Github 构造数据，问题不在于数据太少，而是太多太杂太乱。作者选择了 12 个大项目，大概初始有 90000 PR。这些项目至少是维护完善，有清晰的 contribution guide 并且测试覆盖不错的。以 PR 后的 test cases 为基础，要求 Pre PR 的至少要错一个，然后 Post PR 的至少有一个 fail-to-pass。把这些 Issues 筛出来，最后剩下 2294 个。但这些 Issues 大多数都是在几千个文件的 framework 下得改动数个文件才行。对于 23 年的大模型来说还是有点太困难了。当年最牛的模型 Cluade 3 只能做对 3.79% 的题，现在 Claude 4 已经能做对 67% 了。</p>
<p>为了 Train SWE-Llama，前文提到过，把整个 repo 塞进去既不现实，也不必要。塞什么到 context window 里面，得做 retrieve。作者做了两种 retrive，一个是 Spare Retrival，用 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=252280580&amp;content_type=Article&amp;match_order=1&amp;q=BM25&amp;zhida_source=entity">BM25</a> 在整个 repo 上搜索和 issue 相关的 files，然后尽可能多的塞给 model。另一种是 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=252280580&amp;content_type=Article&amp;match_order=1&amp;q=Oracle+Retrival&amp;zhida_source=entity">Oracle Retrival</a>，这个名字很玄幻，但是大意是说，直接在原本 PR 里面修改过的那些 file 做 retrieve，相当于直接在正确答案上找 context 了。</p>
<h2 id="terminal-bench"><a class="markdownIt-Anchor" href="#terminal-bench"></a> Terminal-Bench</h2>
<p>Terminal-Bench 是一个框架和一组任务，用于评估 AI 代理 <strong>在终端环境中完成复杂任务的能力</strong> 。任务示例包括：1）编译和打包代码存储库；2）下载数据集并训练分类器；3）设置服务器；4）其实还有乱七八糟的编程竞赛-like的题目。</p>
<p>Terminal-Bench 由两部分组成：任务数据集和执行工具。Terminal-Bench 中的每个任务包括：英文描述、Docker 环境、用于验证代理是否成功完成任务的测试脚本、以及解决任务的参考（“Oracle”）解决方案。</p>
<p>怎么说呢，看了几个例子，会在想有些完全是 <code>cat &gt; testl.py &lt;&lt; 'EOF'</code> 这种的符不符合在 CLI 解决问题的定义，但是题目都挺好的，这种开源大家一起构建 Benchmark 的思路也挺开放。就是不知道有多少人是作为素人加入的。</p>
<h2 id="discussion"><a class="markdownIt-Anchor" href="#discussion"></a> Discussion</h2>
<p>调研就这些，但 core problem 是假设我要刷这些 Benchmark，应该怎么搭建环境和 API？怎么搭建 Scaffold？怎么设计 Reward？之后都是改框架的故事了。</p>
<p><strong>在强化学习 (Reinforcement Learning) 中的 Scaffold</strong>：在强化学习中，智能体（Agent）需要在复杂的环境中通过试错来学习最优策略，这通常非常困难，尤其是在<strong>奖励稀疏（Sparse Rewards）<strong>的环境中。Scaffold 的目的就是让“学习”这件事本身变得更容易。在训练初期，为智能体提供</strong>人类专家的操作演示数据</strong>。智能体先通过模仿学习来快速达到一个不错的基线水平，然后再通过自身的探索去超越这个基线。专家的演示就是“脚手架”。</p>
<h3 id="kimi-k2-数据阶段"><a class="markdownIt-Anchor" href="#kimi-k2-数据阶段"></a> KIMI K2 （数据阶段）</h3>
<p>为了让模型学习复杂的工具使用和交互式推理能力，<strong>KIMI K2</strong> 团队建立了一个大规模的智能体数据合成流水线用以 <strong>SFT</strong>，用以模拟真实世界的工具使用场景，生成了数万个高质量的训练样本。该流程主要分为三个阶段：</p>
<p>1）<strong>工具规范生成 (Tool Spec Generation)</strong>：首先，团队构建了一个全面的工具库。这个库不仅包含了从 GitHub 等平台收集的 3000 多个真实世界的工具 (MCP tools)，还通过一个分层的领域生成过程，系统性地进化出了超过 20000 个覆盖金融交易、软件应用、机器人控制等领域的合成工具。</p>
<p>2）<strong>智能体与任务生成 (Agent and Task Generation)</strong>：接着，为工具库中的不同工具组合生成了数千个具有不同能力、专业领域和行为模式的智能体。同时，为每个智能体配置了从简单到复杂不等的任务，并为每个任务都配有明确的成功标准和评估检查点 (Rubric) 。</p>
<p>3）<strong>轨迹生成 (Trajectory Generation)</strong>：最后，通过一个多智能体系统来生成智能体完成任务的轨迹。该系统包含：i) Simulated User（模拟用户）由大语言模型扮演具有不同沟通风格的用户，与智能体进行多轮对话； ii) <strong>工具执行环境</strong>：一个复杂的工具模拟器，负责执行工具调用并提供包括成功、部分失败和边缘案例在内的真实反馈；iii) <strong>质量评估与过滤</strong>：一个基于大语言模型的“裁判”智能体，根据任务的评估标准来判断轨迹是否成功，只有成功的轨迹才会被用于训练；</p>
<p>此外，为了弥补模拟环境与现实世界的差距，团队还引入了真实的执行沙箱（尤其是在编码和软件工程任务中），确保模型能从真实世界的执行反馈中<strong>强化学习</strong>：</p>
<p>1）<strong>大规模沙箱基础设施和真实数据</strong>：为了安全、可扩展地执行模型生成的代码，团队搭建了一个由 Kubernetes 驱动的强大沙箱基础设施 。该系统可以支持超过 10,000 个并发的沙箱实例稳定运行，确保了大规模强化学习的可行性。并且团队从 GitHub 收集了大量的真实世界软件工程任务，例如用户提交的 Issue（问题）和 Pull Request（代码合并请求）。这使得模型的训练目标与实际开发场景高度一致（其实也是和 SWE-Bench 对齐）。</p>
<p>2）<strong>可验证的奖励信号</strong>：单拎出来讲几个我比较感兴趣的点：对于每个软件工程任务（例如修复一个 Bug），环境中都包含了可执行的单元测试（Unit Tests），模型生成的代码会在沙箱环境中被实际执行，并通过这些单元测试进行检验 。奖励信号直接与客观指标挂钩，例如单元测试的通过率。</p>
<p>3）<strong>行为规范</strong>：为了让智能体更高效，团队还引入了“预算控制”机制 。如果模型生成的解决方案（代码）超过了预设的长度限制，会被截断并给予惩罚。这激励模型学会在指定的限制内生成更简洁、高效的解决方案。</p>
<p><strong>其实可以说使用工具的能力（or 知识？）主要来源于 SFT 阶段，RL 阶段更像是利用这些工具培养 SE 的能力。</strong> 还有几个我比较感兴趣的点：</p>
<h4 id="系统性地进化出合成工具"><a class="markdownIt-Anchor" href="#系统性地进化出合成工具"></a> 系统性地“进化”出合成工具</h4>
<p>这指的是一种分层、由总到分的流程，用于创造大量多样化的模拟工具，以确保模型在训练中能接触到尽可能广泛的工具使用场景 1。这个过程不仅仅是随机生成，而是遵循一个系统性的结构：</p>
<ol>
<li>
<p><strong>从关键类别开始</strong>：团队首先定义了一些宏观的核心类别，例如“金融交易”、“软件应用”或“机器人控制”</p>
</li>
<li>
<p><strong>“进化”出具体应用领域</strong>：在每个核心类别下，再衍生出多个更具体的应用领域。例如，在“金融交易”类别下，可以进化出“股票分析”、“期权交易”、“投资组合管理”等具体领域.</p>
</li>
<li>
<p><strong>为领域合成专用工具</strong>：最后，针对每一个具体的应用领域，合成专门的工具。这些工具都带有清晰的接口、功能描述和操作语义，以便模型能够理解和调用。例如，在“股票分析”领域下，可以合成 <code>getStockPrice</code>、<code>getHistoricalData</code>、<code>calculateMovingAverage</code> 等工具。</p>
</li>
</ol>
<p>通过这种从上至下的“进化”过程，团队系统性地生成了超过 20,000 个合成工具，与从真实世界收集的 3000 多个工具相结合，构建了一个覆盖范围极广的工具库，为后续生成多样化的训练任务打下了基础。</p>
<h4 id="评估轨迹生成的正确性"><a class="markdownIt-Anchor" href="#评估轨迹生成的正确性"></a> 评估轨迹生成的正确性</h4>
<p>为了确保合成数据的质量，Kimi K2 团队建立了一套严格的、自动化的评估和过滤机制，其核心可以理解为“<strong>先立规矩，再裁判</strong>”的流程。整个流程是一个多智能体协作的流水线，用于生成和过滤智能体的行为轨迹。评估正确性的具体步骤如下：</p>
<ul>
<li>
<p>预设评估标准 (Rubric)：在生成任何任务时，不仅仅是生成任务本身，还会为每个任务配备一个明确的评估标准（Rubric）。这个标准非常详细，规定了任务的成功条件、预期的工具使用模式以及评估的关键检查点 10。这相当于在考试前就制定好了详细的评分标准。</p>
</li>
<li>
<p>裁判”智能体进行评估：当一个“考生”智能体（Agent）与工具模拟器（Tool Simulator）交互完成任务后，会生成一条完整的行为轨迹。此时，一个专门的、基于大语言模型的“裁判”智能体（Judge Agent）会登场 12。它的唯一工作就是将“考生”智能体的行为轨迹与预设的评估标准（Rubric）进行比对 13。</p>
</li>
<li>
<p>严格过滤与筛选：只有那些被“裁判”智能体判定为完全符合成功标准的轨迹才会被保留下来，用于模型的最终训练，任何不满足成功标准的轨迹都会被丢弃 1515。</p>
</li>
</ul>
<p>这个过程本质上是一种<strong>大规模的拒绝采样 (rejection sampling)</strong>。团队通过自动化流程生成海量的潜在轨迹，然后利用严格的“裁判”机制拒绝掉所有不合格的样本，最终只保留那些经过验证、高质量、可信赖的成功案例。这确保了用于训练的数据不仅多样，而且在逻辑上是正确和可靠的。</p>
<h3 id="kimi-k2训练阶段"><a class="markdownIt-Anchor" href="#kimi-k2训练阶段"></a> KIMI K2（训练阶段）</h3>
<h3 id="训练方法结合可验证奖励与自我批判的强化学习"><a class="markdownIt-Anchor" href="#训练方法结合可验证奖励与自我批判的强化学习"></a> 训练方法：结合可验证奖励与自我批判的强化学习</h3>
<p>Kimi K2 在后训练 (Post-training) 阶段大规模应用了强化学习 (RL) 。团队开发了一个可扩展的强化学习框架，该框架不仅涵盖了具有可验证奖励的任务，还创新性地引入了自我批判奖励机制，以处理更主观、开放的任务 。</p>
<ul>
<li>
<p><strong>可验证奖励的强化学习 (RL with Verifiable Rewards)</strong>：</p>
<ul>
<li><strong>编码与软件工程</strong>：团队收集了大量来自开源数据集和合成来源的编程竞赛题目，并利用预训练数据中的高质量单元测试来确保奖励信号的正确性 。此外，还利用 GitHub 上的拉取请求 (pull requests) 和问题 (issues) 构建了一个包含用户提示和可执行单元测试的软件开发环境 。</li>
<li><strong>数学、STEM 和逻辑任务</strong>：收集了大量高质量问答对，并确保其覆盖范围广泛且难度适中。</li>
</ul>
</li>
<li>
<p><strong>自我批判奖励机制 (RL with Self-Critique Reward)</strong>：对于没有明确正确答案的开放域任务（如创意写作），模型会自我评估其生成的多个回答，并根据一系列核心准则（如清晰度、相关性、对话流畅性等）和预设规则进行排序，从而产生偏好信号用于强化学习 。这种机制使得模型能够将从可验证任务中学到的能力泛化到更广泛的<strong>主观任务</strong>中。</p>
</li>
<li>
<p><strong>智能体部署优化 (Agentic Rollout)</strong>：在强化学习的基础设施中，团队专门针对长程、多轮的智能体任务进行了优化，例如将复杂的环境部署为独立服务，并通过大量并发部署来分摊延迟，以最大化 GPU 利用率 。</p>
</li>
</ul>
<p><strong>架构适配：为智能体任务优化长上下文处理。</strong> 虽然模型架构没有专门为 Agent 设计新的模块，但在参数选择上考虑了智能体应用的需求。智能体应用通常需要高效的长上下文处理能力 。因此，Kimi K2 团队在设计时，选择将注意力头的数量减少到 64 个（作为对比，DeepSeek-V3 为 128 个）。实验表明，虽然增加注意力头数量能带来微小的性能提升（约 0.5% 至 1.2%），但这会导致在长序列（如128k）下的推理开销大幅增加（增加 83%）。为了平衡性能和智能体应用中的长上下文推理成本，团队最终选择了较少的注意力头数量 。</p>
<h3 id="glm-45"><a class="markdownIt-Anchor" href="#glm-45"></a> GLM 4.5</h3>
<ul>
<li><strong>中训练 (Mid-training) 阶段</strong>：模型加入了大规模的合成 Agent 轨迹数据 (Large-scale synthetic agent trajectories) ，中训练我觉得本质上就是学习率变小，把语料变得更专业的 CPT 阶段，还是自监督学习的范畴。</li>
<li><strong>监督微调 (SFT) 阶段</strong> ：搭建了一个四步骤的自动 Agentic 合成数据 Pipeline：1）收集了一系列 Agentic 框架和真实的工具 API ；2）基于这些框架和工具，利用大语言模型自动生成相关的 Agent 任务查询；3）使用现有的大语言模型为合成的任务生成工具调用轨迹；4）多 Judge Agents 评估任务完成情况，只保留成功的轨迹。</li>
<li><strong>Agentic RL 阶段</strong> ：1）<strong>应用场景</strong> 重点关注网页搜索和代码生成等可以自动检查每个动作或答案的 Agentic 场景 (verifiable)，2）<strong>奖励机制</strong> 对于网页搜索任务，使用最终答案的准确性作为整个 Agent 轨迹的奖励。同时，应用 <strong>格式惩罚</strong> (process format penalty)，如果模型未能生成正确的工具调用格式，则该轨迹将获得零奖励。3）<strong>迭代蒸馏</strong>：采用自蒸馏方法，在对初始模型进行强化学习训练后，用其生成的响应替换原始的冷启动数据，从而 train 一个更优的 SFT 模型，在此基础上继续进行 RL 训练。</li>
<li><strong>函数调用强化学习 (Function Calling RL)</strong> ：1）<strong>逐步的基于规则的强化学习</strong>：对于有明确工具调用程序的任务，模型被训练以生成下一步的函数调用或用户响应，并使用基于规则的严格奖励函数来引导模型进行正确的函数调用；2）<strong>端到端的多轮强化学习</strong>：模型首先生成完整的轨迹，然后根据任务完成情况获得奖励 。</li>
</ul>
<h2 id="multi-turn-rl-training-tir"><a class="markdownIt-Anchor" href="#multi-turn-rl-training-tir"></a> Multi-turn RL Training (TIR)</h2>
<h2 id="reward-hacking"><a class="markdownIt-Anchor" href="#reward-hacking"></a> Reward Hacking</h2>
<p>先列一些论文在这里，额之后有机会再看：</p>
<ul>
<li>
<p>在GRPO框架中，LLM可以作为“奖励生成器”（Reward Generator）或“奖励评估器”（Reward Evaluator），用于动态生成或评估奖励信号。例如，LLM可以被训练为一个“奖励模型”（Reward Model），通过输入智能体的行为序列，输出一个奖励分数。</p>
<p>然而，这类方法面临的主要挑战包括：</p>
<ul>
<li><strong>奖励信号的可解释性差</strong>：LLM生成的奖励信号可能难以解释，难以与任务目标对齐。</li>
<li><strong>奖励Hacking风险</strong>：LLM生成的奖励信号可能被智能体“欺骗”或“滥用”，导致智能体通过操纵行为来获取奖励，而忽略任务目标。</li>
<li><strong>Reward Generation via Language Models</strong>. 通过LLM生成奖励信号.</li>
<li><strong>Group Reinforcement Learning with Language Models</strong>. 群体强化学习中的奖励生成与策略优化。</li>
<li><strong>Reward Hacking in Reinforcement Learning</strong>. 系统性地分析了奖励Hacking问题及其应对策略。</li>
<li><strong>Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach</strong>. 在GRPO框架内，使用了一个<strong>多标签奖励回归模型</strong>。这个模型能够预测多个维度的对齐分数（例如，有用性、无害性、诚实性等），然后将这些分数组合成一个单一的奖励信号来指导GRPO训练。</li>
<li><strong>RRM: Robust Reward Model Training Mitigates Reward Hacking.</strong> Reward Hacking的根源在于奖励模型学到了许多与任务无关的虚假相关性（spurious correlations），例如文本长度、格式等。RRM通过 <strong>因果框架</strong> 和 <strong>数据增强</strong> 来迫使奖励模型关注真正与人类偏好相关的因果特征，而忽略这些虚假的上下文无关特征，从而训练出更鲁棒、不易被Hacking 的奖励模型。</li>
<li><strong>Inform: Mitigating Reward Hacking in RLHF via Information-theoretic Reward Modeling</strong>，通过信息论的视角来建模奖励，以减少模型对奖励函数本身的过度拟合</li>
<li><strong>Posterior-GRPO</strong> 提出了一种巧妙的解决方案来应对推理过程中的奖励操控。在代码生成等需要复杂推理链的任务中，如果仅奖励最终结果，则信号过于稀疏；如果奖励中间步骤，模型又可能学会生成看似合理但最终错误的过程。Posterior-GRPO 通过仅对那些最终产出正确结果的推理过程给予奖励，从而确保了过程奖励与最终目标的一致性，有效避免了模型“为了过程而过程”.</li>
<li>**多层GRPO（Multi-Layer GRPO）**通过在一个单一模型内部引入一个双层处理流程，为解决问题提供了新的视角.</li>
<li><strong>GRPO-CARE</strong>. 标准GRPO虽然提升了答案准确率，但推理过程与答案的逻辑一致性却下降了。为此，它设计了一个双层奖励：一层奖励最终答案的正确性，另一层通过一个缓慢演变的参考模型来评估当前推理路径与答案的逻辑一致性，并给予奖励。这种设计迫使模型在追求正确答案的同时，也必须生成一个能够逻辑上支持该答案的连贯推理过程</li>
</ul>
</li>
</ul>
<p><strong>参考资料：</strong></p>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/rlhf/verl/multi-turn/code-walk-through">Awesome-ML-SYS-Tutorial/rlhf/verl/multi-turn/code-walk-through at main · zhaochenyang20/Awesome-ML-SYS-Tutorial</a></p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://simpletir.notion.site/report">SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning | Notion</a></p>
</li>
</ul>
<h3 id="my-thinking"><a class="markdownIt-Anchor" href="#my-thinking"></a> My thinking</h3>
<ol>
<li><strong>使用工具的能力（or 知识？）主要来源于 SFT 阶段，RL 阶段更像是利用这些工具培养解决实际问题的能力。</strong></li>
<li><strong>在合成这些 Agentic 数据时，搭建一个自动化的 Agent （或者说 LLM-based Workflow）去做批量生成是必要的（基于工具的任务生成-&gt;为解决任务的轨迹合成-&gt;基于 Judge Agent 的质量检测）。如果要做创新点的话，2,3步感觉都有做头（比如在不同场景下如何合成轨迹？没有可验证的回答如何质量检测？）</strong></li>
<li><strong>RL 阶段反而比较统一？都是规则+最终结果的奖励机制。K2 虽然有自我批判的奖励机制，但本质上还是在开放域任务上才使用，能 RLVR 尽量 RLVR.</strong></li>
</ol>
<!-- flag of hidden posts -->
  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            天泽龟
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://tzturtle.moe/2025/08/19/benchmark-for-llmagent/">
            https://tzturtle.moe/2025/08/19/benchmark-for-llmagent/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: '4CiiPBXpbjDnPvIIwfuEPEY6-gzGzoHsz',
        appKey: '3AQY35K3Laq9fLvTG2uOHDUT',
        placeholder: '留下你的评论...',
        path: window.location.pathname,
        avatar: 'identicon',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'false',
        highlight: true,
        recordIP: false,
        
        
        
        enableQQ: 'true',
        requiredFields: [],
      });
    };
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E7%A0%94%E7%9B%AE%E6%A0%87"><span class="toc-text"> 调研目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%CF%84-bench"><span class="toc-text"> τττ-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%CF%842-bench"><span class="toc-text"> τ2τ^2τ2-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#swe-bench"><span class="toc-text"> SWE-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#terminal-bench"><span class="toc-text"> Terminal-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion"><span class="toc-text"> Discussion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kimi-k2-%E6%95%B0%E6%8D%AE%E9%98%B6%E6%AE%B5"><span class="toc-text"> KIMI K2 （数据阶段）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%80%A7%E5%9C%B0%E8%BF%9B%E5%8C%96%E5%87%BA%E5%90%88%E6%88%90%E5%B7%A5%E5%85%B7"><span class="toc-text"> 系统性地“进化”出合成工具</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E8%BD%A8%E8%BF%B9%E7%94%9F%E6%88%90%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="toc-text"> 评估轨迹生成的正确性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kimi-k2%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="toc-text"> KIMI K2（训练阶段）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E7%BB%93%E5%90%88%E5%8F%AF%E9%AA%8C%E8%AF%81%E5%A5%96%E5%8A%B1%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text"> 训练方法：结合可验证奖励与自我批判的强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glm-45"><span class="toc-text"> GLM 4.5</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-turn-rl-training-tir"><span class="toc-text"> Multi-turn RL Training (TIR)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reward-hacking"><span class="toc-text"> Reward Hacking</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#my-thinking"><span class="toc-text"> My thinking</span></a></li></ol></li></ol>
</div></main>
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="https://i.loli.net/2021/07/16/yKghkEQWcY34rOU.jpg" 
    class="author-img" 
    alt="author avatar">

<p class="author-name">天泽龟</p>
<p class="author-description">天泽龟的龟是龟裂的龟哦。</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>61</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>5</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>15</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://space.bilibili.com/12645985">
          <i class="iconfont icon-bilibili society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://github.com/TURLEing">
          <i class="iconfont icon-github society-icon"></i>
        </a>
      </div>
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://turleing.github.io/about/">
          <i class="iconfont icon-mail society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E7%A0%94%E7%9B%AE%E6%A0%87"><span class="toc-text"> 调研目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%CF%84-bench"><span class="toc-text"> τττ-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%CF%842-bench"><span class="toc-text"> τ2τ^2τ2-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#swe-bench"><span class="toc-text"> SWE-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#terminal-bench"><span class="toc-text"> Terminal-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion"><span class="toc-text"> Discussion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kimi-k2-%E6%95%B0%E6%8D%AE%E9%98%B6%E6%AE%B5"><span class="toc-text"> KIMI K2 （数据阶段）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%80%A7%E5%9C%B0%E8%BF%9B%E5%8C%96%E5%87%BA%E5%90%88%E6%88%90%E5%B7%A5%E5%85%B7"><span class="toc-text"> 系统性地“进化”出合成工具</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E8%BD%A8%E8%BF%B9%E7%94%9F%E6%88%90%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="toc-text"> 评估轨迹生成的正确性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kimi-k2%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="toc-text"> KIMI K2（训练阶段）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E7%BB%93%E5%90%88%E5%8F%AF%E9%AA%8C%E8%AF%81%E5%A5%96%E5%8A%B1%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text"> 训练方法：结合可验证奖励与自我批判的强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glm-45"><span class="toc-text"> GLM 4.5</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-turn-rl-training-tir"><span class="toc-text"> Multi-turn RL Training (TIR)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reward-hacking"><span class="toc-text"> Reward Hacking</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#my-thinking"><span class="toc-text"> My thinking</span></a></li></ol></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/">
        <div class="categories-list-item">
          算法竞赛
          <span class="categories-list-item-badge">12</span>
        </div>
      </a>
    
      <a href="/categories/%E7%94%9F%E6%B4%BB%E5%88%86%E4%BA%AB/">
        <div class="categories-list-item">
          生活分享
          <span class="categories-list-item-badge">10</span>
        </div>
      </a>
    
      <a href="/categories/%E4%B8%93%E4%B8%9A%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          专业学习
          <span class="categories-list-item-badge">27</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/">
        <div class="categories-list-item">
          科研学习
          <span class="categories-list-item-badge">8</span>
        </div>
      </a>
    
      <a href="/categories/%E7%A4%BE%E5%9B%A2%E6%8E%A8%E9%80%81/">
        <div class="categories-list-item">
          社团推送
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" 
        title="计算机组成原理">
        <div class="tags-list-item">计算机组成原理</div>
      </a>
    
      <a 
        href="/tags/%E9%9A%8F%E7%AC%94/" 
        title="随笔">
        <div class="tags-list-item">随笔</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" 
        title="强化学习">
        <div class="tags-list-item">强化学习</div>
      </a>
    
      <a 
        href="/tags/NLP/" 
        title="NLP">
        <div class="tags-list-item">NLP</div>
      </a>
    
      <a 
        href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" 
        title="操作系统">
        <div class="tags-list-item">操作系统</div>
      </a>
    
      <a 
        href="/tags/%E6%97%A5%E9%BA%BB/" 
        title="日麻">
        <div class="tags-list-item">日麻</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" 
        title="大模型">
        <div class="tags-list-item">大模型</div>
      </a>
    
      <a 
        href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" 
        title="字符串">
        <div class="tags-list-item">字符串</div>
      </a>
    
      <a 
        href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" 
        title="分布式">
        <div class="tags-list-item">分布式</div>
      </a>
    
      <a 
        href="/tags/%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97/" 
        title="情感计算">
        <div class="tags-list-item">情感计算</div>
      </a>
    
      <a 
        href="/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F/" 
        title="多项式">
        <div class="tags-list-item">多项式</div>
      </a>
    
      <a 
        href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" 
        title="推荐系统">
        <div class="tags-list-item">推荐系统</div>
      </a>
    
      <a 
        href="/tags/%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F/" 
        title="强连通分量">
        <div class="tags-list-item">强连通分量</div>
      </a>
    
      <a 
        href="/tags/%E7%BA%BF%E6%80%A7%E5%9F%BA/" 
        title="线性基">
        <div class="tags-list-item">线性基</div>
      </a>
    
      <a 
        href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" 
        title="博弈论">
        <div class="tags-list-item">博弈论</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E7%A0%94%E7%9B%AE%E6%A0%87"><span class="toc-text"> 调研目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%CF%84-bench"><span class="toc-text"> τττ-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%CF%842-bench"><span class="toc-text"> τ2τ^2τ2-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#swe-bench"><span class="toc-text"> SWE-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#terminal-bench"><span class="toc-text"> Terminal-Bench</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion"><span class="toc-text"> Discussion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kimi-k2-%E6%95%B0%E6%8D%AE%E9%98%B6%E6%AE%B5"><span class="toc-text"> KIMI K2 （数据阶段）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%80%A7%E5%9C%B0%E8%BF%9B%E5%8C%96%E5%87%BA%E5%90%88%E6%88%90%E5%B7%A5%E5%85%B7"><span class="toc-text"> 系统性地“进化”出合成工具</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E8%BD%A8%E8%BF%B9%E7%94%9F%E6%88%90%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="toc-text"> 评估轨迹生成的正确性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kimi-k2%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="toc-text"> KIMI K2（训练阶段）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E7%BB%93%E5%90%88%E5%8F%AF%E9%AA%8C%E8%AF%81%E5%A5%96%E5%8A%B1%E4%B8%8E%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text"> 训练方法：结合可验证奖励与自我批判的强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glm-45"><span class="toc-text"> GLM 4.5</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-turn-rl-training-tir"><span class="toc-text"> Multi-turn RL Training (TIR)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reward-hacking"><span class="toc-text"> Reward Hacking</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#my-thinking"><span class="toc-text"> My thinking</span></a></li></ol></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-09-28</div>
        <a href="/2025/09/28/CS336/"><div class="recent-posts-item-content">CS336 学习笔记 Part 1：从模型架构变体到底层硬件优化</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-08-15</div>
        <a href="/2025/08/15/my-interest-in-ag/"><div class="recent-posts-item-content">现代大模型时代下的情感计算综述</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-07-21</div>
        <a href="/2025/07/21/Rl-reproduce/"><div class="recent-posts-item-content">RL 实验复现随笔【Tool Agent】【PPO、GRPO】</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2025-07-16</div>
        <a href="/2025/07/16/daily-story-pyenv-conda/"><div class="recent-posts-item-content">【实验室小品一则】什么是pyenv，什么是miniconda</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020 -
          
          2025
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          天泽龟的龟壳屋
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
      <div class="footer-dsc">
        
          本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer> 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton" 
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
      

  
  
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">

  
 
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
    
      <script> 
        loadScript('/js/lib/busuanzi.min.js') 
      </script>
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
     
    
    
  </body>
</html>
